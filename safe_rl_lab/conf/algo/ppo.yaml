name: "PPO"

#algo hyperparams
lr: 3e-4
anneal_lr: false
clip_epsilon: 0.2
entropy_coef: 0.001
max_grad_norm: 0.5
normalize_adv: true

#PPO specifics
update_epochs: 8
batch_size: 64
rollout_size: 256

gae:
  gamma: 0.99
  lam: 0.95

model_arch: "disjoint"
hidden_sizes: [256, 256]
activation: "tanh"
initialization_method: "orthogonal"

#early stopping
use_kl_early_stopping: true
early_stopping_target_kl: 0.01

# Logic Flags (Specific to Standard PPO)
use_cost: false
use_phasic: false