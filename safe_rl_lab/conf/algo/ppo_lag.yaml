name: "PPO_Lag"

#algo hyperparams
lr: 1e-4
anneal_lr: false
clip_epsilon: 0.2
entropy_coef: 0.0
max_grad_norm: 0.5
normalize_adv: true

#PPO Lag specifics
cost_limit: 25
update_epochs: 8
batch_size: 64
rollout_size: 512
k_i: 0.001
k_p: 1.0
k_d: 0.0
d_delay: 15
pid_delta_p_ema_alpha: 0.95
pid_delta_d_ema_alpha: 0.90
sum_norm: true, #if adv_total are scaled, the lambda should not be capped
diff_norm: false,
penalty_max: 100.0
lagrangian_multiplier_init: 0.0

gae:
  gamma: 0.99
  lam: 0.95

model_arch: "disjoint"
hidden_sizes: [256, 256]
activation: "tanh"
initialization_method: "orthogonal"

#early stopping
use_kl_early_stopping: true
early_stopping_target_kl: 0.01

use_cost: true